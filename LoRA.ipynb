{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Rank Adaptation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torch==2.0.1+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    DEVICE=torch.device('cuda')\n",
    "else:\n",
    "    DEVICE=torch.device('cpu')\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    \"\"\"\n",
    "    NVIDIA CUDA Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev=1/torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A=nn.Parameter(torch.randn(in_dim, rank)*std_dev)\n",
    "        self.B=nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha=alpha\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.alpha*(x@self.A@self.B) \n",
    "        # Here, @ denotes matrix multiplication\n",
    "        return x\n",
    "    \n",
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear=linear\n",
    "        self.lora=LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)+self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "# random_seed=123\n",
    "\n",
    "# torch.manual_seed(random_seed)\n",
    "# layer=nn.Linear(10,2)\n",
    "# x=torch.randn((1, 10))\n",
    "\n",
    "# print(x)\n",
    "# print(layer)\n",
    "# print('Original output:', layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LoRA to Linear Layer\n",
    "Let's apply LoRA to the Linear layer, we see that the results are the same since we haven't trained the LoRA weights yet. In other words, everything works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4645, 0.3197]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Applying LoRA to Linear Layer\n",
    "layer=nn.Linear(10,2)\n",
    "x=torch.randn((1, 10))\n",
    "layer_lora_1=LinearWithLoRA(layer, rank=2, alpha=4)\n",
    "print(layer_lora_1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# This LoRA code is equivalent to LinearWithLoRA\n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lora=self.lora.A @ self.lora.B # combine LoRA metrices\n",
    "        # then combine LoRA original weights\n",
    "        combined_weight = self.linear.weight + self.lora.alpha*lora.T\n",
    "        return F.linear(x, combined_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4645, 0.3197]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_lora_2=LinearWithLoRAMerged(layer, rank=2, alpha=4)\n",
    "print(layer_lora_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\n",
    "num_features=784\n",
    "num_hidden_1=128\n",
    "num_hidden_2=256\n",
    "num_classes=10\n",
    "\n",
    "# Settings\n",
    "DEVICE=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate=0.005\n",
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.005\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.layers(x)\n",
    "        return x\n",
    "    \n",
    "model=MultilayerPerceptron(\n",
    "    num_features = num_features,\n",
    "    num_hidden_1 = num_hidden_1,\n",
    "    num_hidden_2 = num_hidden_2,\n",
    "    num_classes = num_classes\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "optimizer_pretrained=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(DEVICE)\n",
    "print(model)\n",
    "print(optimizer_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE=64\n",
    "\n",
    "# Note: transforms.ToTensor() scales input images to 0-1 range\n",
    "train_dataset=datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset=datasets.MNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "train_loader=DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader=DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples=0,0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features=features.view(-1, 28*28).to(device)\n",
    "            targets=targets.to(device)\n",
    "            logits=model(features)\n",
    "            _, predicted_labels=torch.max(logits,1)\n",
    "            num_examples+=targets.size(0)\n",
    "            correct_pred+=(predicted_labels==targets).sum()\n",
    "        return correct_pred.float()/num_examples*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010|Batch 000/938| Loss: 2.3076\n",
      "Epoch: 001/010|Batch 400/938| Loss: 0.1613\n",
      "Epoch: 001/010|Batch 800/938| Loss: 0.1605\n",
      "Epoch: 001/010 training accuracy: 96.76%\n",
      "Time elapsed: 0.17 min\n",
      "Epoch: 002/010|Batch 000/938| Loss: 0.1003\n",
      "Epoch: 002/010|Batch 400/938| Loss: 0.0263\n",
      "Epoch: 002/010|Batch 800/938| Loss: 0.0424\n",
      "Epoch: 002/010 training accuracy: 97.36%\n",
      "Time elapsed: 0.33 min\n",
      "Epoch: 003/010|Batch 000/938| Loss: 0.0978\n",
      "Epoch: 003/010|Batch 400/938| Loss: 0.1006\n",
      "Epoch: 003/010|Batch 800/938| Loss: 0.1845\n",
      "Epoch: 003/010 training accuracy: 97.83%\n",
      "Time elapsed: 0.48 min\n",
      "Epoch: 004/010|Batch 000/938| Loss: 0.2168\n",
      "Epoch: 004/010|Batch 400/938| Loss: 0.1372\n",
      "Epoch: 004/010|Batch 800/938| Loss: 0.0543\n",
      "Epoch: 004/010 training accuracy: 98.26%\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 005/010|Batch 000/938| Loss: 0.0336\n",
      "Epoch: 005/010|Batch 400/938| Loss: 0.0326\n",
      "Epoch: 005/010|Batch 800/938| Loss: 0.0873\n",
      "Epoch: 005/010 training accuracy: 97.96%\n",
      "Time elapsed: 0.79 min\n",
      "Epoch: 006/010|Batch 000/938| Loss: 0.0156\n",
      "Epoch: 006/010|Batch 400/938| Loss: 0.0954\n",
      "Epoch: 006/010|Batch 800/938| Loss: 0.0065\n",
      "Epoch: 006/010 training accuracy: 98.29%\n",
      "Time elapsed: 0.95 min\n",
      "Epoch: 007/010|Batch 000/938| Loss: 0.0680\n",
      "Epoch: 007/010|Batch 400/938| Loss: 0.0276\n",
      "Epoch: 007/010|Batch 800/938| Loss: 0.0007\n",
      "Epoch: 007/010 training accuracy: 98.57%\n",
      "Time elapsed: 1.10 min\n",
      "Epoch: 008/010|Batch 000/938| Loss: 0.0053\n",
      "Epoch: 008/010|Batch 400/938| Loss: 0.2415\n",
      "Epoch: 008/010|Batch 800/938| Loss: 0.0652\n",
      "Epoch: 008/010 training accuracy: 98.85%\n",
      "Time elapsed: 1.26 min\n",
      "Epoch: 009/010|Batch 000/938| Loss: 0.0073\n",
      "Epoch: 009/010|Batch 400/938| Loss: 0.0164\n",
      "Epoch: 009/010|Batch 800/938| Loss: 0.0042\n",
      "Epoch: 009/010 training accuracy: 98.60%\n",
      "Time elapsed: 1.40 min\n",
      "Epoch: 010/010|Batch 000/938| Loss: 0.0307\n",
      "Epoch: 010/010|Batch 400/938| Loss: 0.1181\n",
      "Epoch: 010/010|Batch 800/938| Loss: 0.0193\n",
      "Epoch: 010/010 training accuracy: 98.37%\n",
      "Time elapsed: 1.55 min\n",
      "Total Training Time: 1.55 min\n",
      "Test accuracy: 96.81%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train(num_epochs, model, optimizer, train_loader, device):\n",
    "    start_time=time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            features=features.view(-1, 28*28).to(device)\n",
    "            targets=targets.to(device)\n",
    "            \n",
    "            # forward and back propagation\n",
    "            logits=model(features)\n",
    "            loss=F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # logging\n",
    "            if not batch_idx %400:\n",
    "                print('Epoch: %03d/%03d|Batch %03d/%03d| Loss: %.4f' % (epoch+1, num_epochs, batch_idx, len(train_loader), loss))\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            print('Epoch: %03d/%03d training accuracy: %.2f%%' % (epoch+1, num_epochs, compute_accuracy(model, train_loader, device)))\n",
    "        \n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "                  \n",
    "                  \n",
    "train(num_epochs, model, optimizer_pretrained, train_loader, DEVICE)\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "model_lora=copy.deepcopy(model)\n",
    "\n",
    "model_lora.layers[0]=LinearWithLoRAMerged(model_lora.layers[0], rank=4, alpha=8)\n",
    "model_lora.layers[2]=LinearWithLoRAMerged(model_lora.layers[2], rank=4, alpha=8)\n",
    "model_lora.layers[4]=LinearWithLoRAMerged(model_lora.layers[4], rank=4, alpha=8)\n",
    "model_lora.to(DEVICE)\n",
    "optimizer_lora=torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
    "print(model_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy orig model:96.81%\n",
      "Test accuracy LoRA model:96.81%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test accuracy orig model:{compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
    "print(f'Test accuracy LoRA model:{compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.linear.weight:False\n",
      "layers.0.linear.bias:False\n",
      "layers.0.lora.A:True\n",
      "layers.0.lora.B:True\n",
      "layers.2.linear.weight:False\n",
      "layers.2.linear.bias:False\n",
      "layers.2.lora.A:True\n",
      "layers.2.lora.B:True\n",
      "layers.4.linear.weight:False\n",
      "layers.4.linear.bias:False\n",
      "layers.4.lora.A:True\n",
      "layers.4.lora.B:True\n"
     ]
    }
   ],
   "source": [
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad=False\n",
    "        else:\n",
    "            # recursively freeze linear layers in children modules\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "freeze_linear_layers(model_lora)\n",
    "for name, param in model_lora.named_parameters():\n",
    "    print(f'{name}:{param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010|Batch 000/938| Loss: 0.0253\n",
      "Epoch: 001/010|Batch 400/938| Loss: 0.0189\n",
      "Epoch: 001/010|Batch 800/938| Loss: 0.1044\n",
      "Epoch: 001/010 training accuracy: 98.33%\n",
      "Time elapsed: 0.18 min\n",
      "Epoch: 002/010|Batch 000/938| Loss: 0.0302\n",
      "Epoch: 002/010|Batch 400/938| Loss: 0.0162\n",
      "Epoch: 002/010|Batch 800/938| Loss: 0.0117\n",
      "Epoch: 002/010 training accuracy: 99.02%\n",
      "Time elapsed: 0.35 min\n",
      "Epoch: 003/010|Batch 000/938| Loss: 0.0307\n",
      "Epoch: 003/010|Batch 400/938| Loss: 0.0063\n",
      "Epoch: 003/010|Batch 800/938| Loss: 0.0574\n",
      "Epoch: 003/010 training accuracy: 98.81%\n",
      "Time elapsed: 0.54 min\n",
      "Epoch: 004/010|Batch 000/938| Loss: 0.0488\n",
      "Epoch: 004/010|Batch 400/938| Loss: 0.0111\n",
      "Epoch: 004/010|Batch 800/938| Loss: 0.0052\n",
      "Epoch: 004/010 training accuracy: 98.99%\n",
      "Time elapsed: 0.72 min\n",
      "Epoch: 005/010|Batch 000/938| Loss: 0.0004\n",
      "Epoch: 005/010|Batch 400/938| Loss: 0.0139\n",
      "Epoch: 005/010|Batch 800/938| Loss: 0.0059\n",
      "Epoch: 005/010 training accuracy: 98.89%\n",
      "Time elapsed: 0.90 min\n",
      "Epoch: 006/010|Batch 000/938| Loss: 0.0040\n",
      "Epoch: 006/010|Batch 400/938| Loss: 0.0178\n",
      "Epoch: 006/010|Batch 800/938| Loss: 0.0241\n",
      "Epoch: 006/010 training accuracy: 98.69%\n",
      "Time elapsed: 1.07 min\n",
      "Epoch: 007/010|Batch 000/938| Loss: 0.0092\n",
      "Epoch: 007/010|Batch 400/938| Loss: 0.0744\n",
      "Epoch: 007/010|Batch 800/938| Loss: 0.0664\n",
      "Epoch: 007/010 training accuracy: 99.14%\n",
      "Time elapsed: 1.23 min\n",
      "Epoch: 008/010|Batch 000/938| Loss: 0.0018\n",
      "Epoch: 008/010|Batch 400/938| Loss: 0.0009\n",
      "Epoch: 008/010|Batch 800/938| Loss: 0.0004\n",
      "Epoch: 008/010 training accuracy: 99.33%\n",
      "Time elapsed: 1.41 min\n",
      "Epoch: 009/010|Batch 000/938| Loss: 0.0290\n",
      "Epoch: 009/010|Batch 400/938| Loss: 0.0507\n",
      "Epoch: 009/010|Batch 800/938| Loss: 0.0025\n",
      "Epoch: 009/010 training accuracy: 99.04%\n",
      "Time elapsed: 1.58 min\n",
      "Epoch: 010/010|Batch 000/938| Loss: 0.0028\n",
      "Epoch: 010/010|Batch 400/938| Loss: 0.1078\n",
      "Epoch: 010/010|Batch 800/938| Loss: 0.0004\n",
      "Epoch: 010/010 training accuracy: 98.58%\n",
      "Time elapsed: 1.75 min\n",
      "Total Training Time: 1.75 min\n",
      "Test accuracy LoRA finetune: 96.71%\n"
     ]
    }
   ],
   "source": [
    "optimizer_lora=torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
    "train(num_epochs, model_lora, optimizer_lora, train_loader, DEVICE)\n",
    "print(f'Test accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy orig model:96.81%\n",
      "Test accuracy LoRA model:96.71%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test accuracy orig model:{compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
    "print(f'Test accuracy LoRA model:{compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
